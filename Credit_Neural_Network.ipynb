{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMy+nDpX4GAfpwcSK7fikT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luscaswolf/Credit_Neural-Network/blob/main/Credit_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "gYnn3p9FmyrW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gyCbwyg-wkqT"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('credit.pkl', 'rb') as f:\n",
        "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZJQ3JiXm-CZ",
        "outputId": "43ea202d-a13c-4f7a-e077-8c630273bab0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtjX2ER3nEek",
        "outputId": "06fa74f5-659d-4fe1-f1e6-5b83a091b3a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(3 + 1) / 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RGESYEIxdzw",
        "outputId": "a2d831f7-2b04-4a0f-be9d-e8b1c51b700a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 -> 100 -> 100 -> 1\n",
        "# 3 -> 2 -> 2 -> 1\n",
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.00000100, solver='adam', activation = 'relu', hidden_layer_sizes=(2,2))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tip89PwUpjfo",
        "outputId": "8e5a2cb3-20dc-45b8-d380-56619ffcd5f6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.89862625\n",
            "Iteration 2, loss = 0.89281916\n",
            "Iteration 3, loss = 0.88652842\n",
            "Iteration 4, loss = 0.87963990\n",
            "Iteration 5, loss = 0.87276785\n",
            "Iteration 6, loss = 0.86572567\n",
            "Iteration 7, loss = 0.85825847\n",
            "Iteration 8, loss = 0.85181446\n",
            "Iteration 9, loss = 0.84461676\n",
            "Iteration 10, loss = 0.83803146\n",
            "Iteration 11, loss = 0.83123307\n",
            "Iteration 12, loss = 0.82471238\n",
            "Iteration 13, loss = 0.81789495\n",
            "Iteration 14, loss = 0.81126798\n",
            "Iteration 15, loss = 0.80456356\n",
            "Iteration 16, loss = 0.79796363\n",
            "Iteration 17, loss = 0.79131154\n",
            "Iteration 18, loss = 0.78472357\n",
            "Iteration 19, loss = 0.77828253\n",
            "Iteration 20, loss = 0.77185882\n",
            "Iteration 21, loss = 0.76559589\n",
            "Iteration 22, loss = 0.75940864\n",
            "Iteration 23, loss = 0.75319865\n",
            "Iteration 24, loss = 0.74713691\n",
            "Iteration 25, loss = 0.74123154\n",
            "Iteration 26, loss = 0.73553354\n",
            "Iteration 27, loss = 0.72967398\n",
            "Iteration 28, loss = 0.72403829\n",
            "Iteration 29, loss = 0.71828980\n",
            "Iteration 30, loss = 0.71268573\n",
            "Iteration 31, loss = 0.70705305\n",
            "Iteration 32, loss = 0.70148408\n",
            "Iteration 33, loss = 0.69606872\n",
            "Iteration 34, loss = 0.69062868\n",
            "Iteration 35, loss = 0.68530934\n",
            "Iteration 36, loss = 0.68007951\n",
            "Iteration 37, loss = 0.67501752\n",
            "Iteration 38, loss = 0.67006586\n",
            "Iteration 39, loss = 0.66525289\n",
            "Iteration 40, loss = 0.66031660\n",
            "Iteration 41, loss = 0.65553263\n",
            "Iteration 42, loss = 0.65083951\n",
            "Iteration 43, loss = 0.64618079\n",
            "Iteration 44, loss = 0.63933759\n",
            "Iteration 45, loss = 0.62999284\n",
            "Iteration 46, loss = 0.62018688\n",
            "Iteration 47, loss = 0.61039727\n",
            "Iteration 48, loss = 0.60092352\n",
            "Iteration 49, loss = 0.59179183\n",
            "Iteration 50, loss = 0.58276864\n",
            "Iteration 51, loss = 0.57391907\n",
            "Iteration 52, loss = 0.56503498\n",
            "Iteration 53, loss = 0.55629966\n",
            "Iteration 54, loss = 0.54770151\n",
            "Iteration 55, loss = 0.53926220\n",
            "Iteration 56, loss = 0.53099821\n",
            "Iteration 57, loss = 0.52273862\n",
            "Iteration 58, loss = 0.51477960\n",
            "Iteration 59, loss = 0.50661138\n",
            "Iteration 60, loss = 0.49866063\n",
            "Iteration 61, loss = 0.49073264\n",
            "Iteration 62, loss = 0.48284570\n",
            "Iteration 63, loss = 0.47500792\n",
            "Iteration 64, loss = 0.46704439\n",
            "Iteration 65, loss = 0.45923984\n",
            "Iteration 66, loss = 0.45149275\n",
            "Iteration 67, loss = 0.44377354\n",
            "Iteration 68, loss = 0.43629321\n",
            "Iteration 69, loss = 0.42870945\n",
            "Iteration 70, loss = 0.42151546\n",
            "Iteration 71, loss = 0.41444088\n",
            "Iteration 72, loss = 0.40762825\n",
            "Iteration 73, loss = 0.40100237\n",
            "Iteration 74, loss = 0.39449424\n",
            "Iteration 75, loss = 0.38800552\n",
            "Iteration 76, loss = 0.38154539\n",
            "Iteration 77, loss = 0.37530268\n",
            "Iteration 78, loss = 0.36919902\n",
            "Iteration 79, loss = 0.36334145\n",
            "Iteration 80, loss = 0.35738958\n",
            "Iteration 81, loss = 0.35152495\n",
            "Iteration 82, loss = 0.34558507\n",
            "Iteration 83, loss = 0.33966918\n",
            "Iteration 84, loss = 0.33386411\n",
            "Iteration 85, loss = 0.32789047\n",
            "Iteration 86, loss = 0.32228815\n",
            "Iteration 87, loss = 0.31685166\n",
            "Iteration 88, loss = 0.31184533\n",
            "Iteration 89, loss = 0.30693133\n",
            "Iteration 90, loss = 0.30227840\n",
            "Iteration 91, loss = 0.29774388\n",
            "Iteration 92, loss = 0.29319251\n",
            "Iteration 93, loss = 0.28901621\n",
            "Iteration 94, loss = 0.28495761\n",
            "Iteration 95, loss = 0.28097282\n",
            "Iteration 96, loss = 0.27727535\n",
            "Iteration 97, loss = 0.27362328\n",
            "Iteration 98, loss = 0.27009981\n",
            "Iteration 99, loss = 0.26685557\n",
            "Iteration 100, loss = 0.26359159\n",
            "Iteration 101, loss = 0.26020984\n",
            "Iteration 102, loss = 0.25692635\n",
            "Iteration 103, loss = 0.25370219\n",
            "Iteration 104, loss = 0.25074474\n",
            "Iteration 105, loss = 0.24787005\n",
            "Iteration 106, loss = 0.24510789\n",
            "Iteration 107, loss = 0.24244161\n",
            "Iteration 108, loss = 0.23994301\n",
            "Iteration 109, loss = 0.23739927\n",
            "Iteration 110, loss = 0.23499281\n",
            "Iteration 111, loss = 0.23258594\n",
            "Iteration 112, loss = 0.23032134\n",
            "Iteration 113, loss = 0.22828216\n",
            "Iteration 114, loss = 0.22625230\n",
            "Iteration 115, loss = 0.22425623\n",
            "Iteration 116, loss = 0.22227369\n",
            "Iteration 117, loss = 0.22042837\n",
            "Iteration 118, loss = 0.21844966\n",
            "Iteration 119, loss = 0.21662945\n",
            "Iteration 120, loss = 0.21484557\n",
            "Iteration 121, loss = 0.21304377\n",
            "Iteration 122, loss = 0.21132404\n",
            "Iteration 123, loss = 0.20969928\n",
            "Iteration 124, loss = 0.20813627\n",
            "Iteration 125, loss = 0.20653147\n",
            "Iteration 126, loss = 0.20506174\n",
            "Iteration 127, loss = 0.20365202\n",
            "Iteration 128, loss = 0.20236121\n",
            "Iteration 129, loss = 0.20112372\n",
            "Iteration 130, loss = 0.19989865\n",
            "Iteration 131, loss = 0.19873410\n",
            "Iteration 132, loss = 0.19765537\n",
            "Iteration 133, loss = 0.19654299\n",
            "Iteration 134, loss = 0.19551598\n",
            "Iteration 135, loss = 0.19455240\n",
            "Iteration 136, loss = 0.19360833\n",
            "Iteration 137, loss = 0.19266067\n",
            "Iteration 138, loss = 0.19174974\n",
            "Iteration 139, loss = 0.19090102\n",
            "Iteration 140, loss = 0.19009891\n",
            "Iteration 141, loss = 0.18931135\n",
            "Iteration 142, loss = 0.18854326\n",
            "Iteration 143, loss = 0.18781468\n",
            "Iteration 144, loss = 0.18706950\n",
            "Iteration 145, loss = 0.18641364\n",
            "Iteration 146, loss = 0.18572990\n",
            "Iteration 147, loss = 0.18508896\n",
            "Iteration 148, loss = 0.18444271\n",
            "Iteration 149, loss = 0.18379874\n",
            "Iteration 150, loss = 0.18324789\n",
            "Iteration 151, loss = 0.18263019\n",
            "Iteration 152, loss = 0.18203830\n",
            "Iteration 153, loss = 0.18149619\n",
            "Iteration 154, loss = 0.18095021\n",
            "Iteration 155, loss = 0.18046761\n",
            "Iteration 156, loss = 0.17994078\n",
            "Iteration 157, loss = 0.17940250\n",
            "Iteration 158, loss = 0.17898613\n",
            "Iteration 159, loss = 0.17849072\n",
            "Iteration 160, loss = 0.17802727\n",
            "Iteration 161, loss = 0.17754178\n",
            "Iteration 162, loss = 0.17708665\n",
            "Iteration 163, loss = 0.17662059\n",
            "Iteration 164, loss = 0.17619909\n",
            "Iteration 165, loss = 0.17574941\n",
            "Iteration 166, loss = 0.17536033\n",
            "Iteration 167, loss = 0.17498585\n",
            "Iteration 168, loss = 0.17453347\n",
            "Iteration 169, loss = 0.17413131\n",
            "Iteration 170, loss = 0.17376490\n",
            "Iteration 171, loss = 0.17331586\n",
            "Iteration 172, loss = 0.17290628\n",
            "Iteration 173, loss = 0.17253383\n",
            "Iteration 174, loss = 0.17216002\n",
            "Iteration 175, loss = 0.17176694\n",
            "Iteration 176, loss = 0.17141555\n",
            "Iteration 177, loss = 0.17123990\n",
            "Iteration 178, loss = 0.17070725\n",
            "Iteration 179, loss = 0.17034589\n",
            "Iteration 180, loss = 0.17001938\n",
            "Iteration 181, loss = 0.16963423\n",
            "Iteration 182, loss = 0.16930381\n",
            "Iteration 183, loss = 0.16895466\n",
            "Iteration 184, loss = 0.16861333\n",
            "Iteration 185, loss = 0.16828812\n",
            "Iteration 186, loss = 0.16798100\n",
            "Iteration 187, loss = 0.16762592\n",
            "Iteration 188, loss = 0.16735840\n",
            "Iteration 189, loss = 0.16699613\n",
            "Iteration 190, loss = 0.16671701\n",
            "Iteration 191, loss = 0.16637035\n",
            "Iteration 192, loss = 0.16608007\n",
            "Iteration 193, loss = 0.16577263\n",
            "Iteration 194, loss = 0.16548699\n",
            "Iteration 195, loss = 0.16518267\n",
            "Iteration 196, loss = 0.16490441\n",
            "Iteration 197, loss = 0.16461142\n",
            "Iteration 198, loss = 0.16431313\n",
            "Iteration 199, loss = 0.16403660\n",
            "Iteration 200, loss = 0.16377919\n",
            "Iteration 201, loss = 0.16347227\n",
            "Iteration 202, loss = 0.16324648\n",
            "Iteration 203, loss = 0.16296324\n",
            "Iteration 204, loss = 0.16267587\n",
            "Iteration 205, loss = 0.16241726\n",
            "Iteration 206, loss = 0.16215893\n",
            "Iteration 207, loss = 0.16188448\n",
            "Iteration 208, loss = 0.16161590\n",
            "Iteration 209, loss = 0.16142950\n",
            "Iteration 210, loss = 0.16111616\n",
            "Iteration 211, loss = 0.16086629\n",
            "Iteration 212, loss = 0.16058927\n",
            "Iteration 213, loss = 0.16032361\n",
            "Iteration 214, loss = 0.16010443\n",
            "Iteration 215, loss = 0.15986479\n",
            "Iteration 216, loss = 0.15956849\n",
            "Iteration 217, loss = 0.15932715\n",
            "Iteration 218, loss = 0.15911117\n",
            "Iteration 219, loss = 0.15884939\n",
            "Iteration 220, loss = 0.15860343\n",
            "Iteration 221, loss = 0.15837572\n",
            "Iteration 222, loss = 0.15815331\n",
            "Iteration 223, loss = 0.15792359\n",
            "Iteration 224, loss = 0.15776336\n",
            "Iteration 225, loss = 0.15750443\n",
            "Iteration 226, loss = 0.15720589\n",
            "Iteration 227, loss = 0.15695683\n",
            "Iteration 228, loss = 0.15672225\n",
            "Iteration 229, loss = 0.15651160\n",
            "Iteration 230, loss = 0.15628209\n",
            "Iteration 231, loss = 0.15605098\n",
            "Iteration 232, loss = 0.15582559\n",
            "Iteration 233, loss = 0.15562111\n",
            "Iteration 234, loss = 0.15538815\n",
            "Iteration 235, loss = 0.15517356\n",
            "Iteration 236, loss = 0.15500145\n",
            "Iteration 237, loss = 0.15472960\n",
            "Iteration 238, loss = 0.15448750\n",
            "Iteration 239, loss = 0.15434836\n",
            "Iteration 240, loss = 0.15407902\n",
            "Iteration 241, loss = 0.15385353\n",
            "Iteration 242, loss = 0.15367304\n",
            "Iteration 243, loss = 0.15342294\n",
            "Iteration 244, loss = 0.15319258\n",
            "Iteration 245, loss = 0.15298863\n",
            "Iteration 246, loss = 0.15278566\n",
            "Iteration 247, loss = 0.15255161\n",
            "Iteration 248, loss = 0.15233062\n",
            "Iteration 249, loss = 0.15213576\n",
            "Iteration 250, loss = 0.15188197\n",
            "Iteration 251, loss = 0.15163903\n",
            "Iteration 252, loss = 0.15142430\n",
            "Iteration 253, loss = 0.15121866\n",
            "Iteration 254, loss = 0.15098092\n",
            "Iteration 255, loss = 0.15076322\n",
            "Iteration 256, loss = 0.15056546\n",
            "Iteration 257, loss = 0.15036502\n",
            "Iteration 258, loss = 0.15012217\n",
            "Iteration 259, loss = 0.14991290\n",
            "Iteration 260, loss = 0.14967854\n",
            "Iteration 261, loss = 0.14945979\n",
            "Iteration 262, loss = 0.14924024\n",
            "Iteration 263, loss = 0.14901258\n",
            "Iteration 264, loss = 0.14880141\n",
            "Iteration 265, loss = 0.14859653\n",
            "Iteration 266, loss = 0.14838404\n",
            "Iteration 267, loss = 0.14816344\n",
            "Iteration 268, loss = 0.14795110\n",
            "Iteration 269, loss = 0.14772575\n",
            "Iteration 270, loss = 0.14749045\n",
            "Iteration 271, loss = 0.14729275\n",
            "Iteration 272, loss = 0.14706369\n",
            "Iteration 273, loss = 0.14685937\n",
            "Iteration 274, loss = 0.14664397\n",
            "Iteration 275, loss = 0.14646268\n",
            "Iteration 276, loss = 0.14619516\n",
            "Iteration 277, loss = 0.14600085\n",
            "Iteration 278, loss = 0.14577472\n",
            "Iteration 279, loss = 0.14558449\n",
            "Iteration 280, loss = 0.14535702\n",
            "Iteration 281, loss = 0.14513925\n",
            "Iteration 282, loss = 0.14494129\n",
            "Iteration 283, loss = 0.14476616\n",
            "Iteration 284, loss = 0.14453336\n",
            "Iteration 285, loss = 0.14431646\n",
            "Iteration 286, loss = 0.14409664\n",
            "Iteration 287, loss = 0.14392787\n",
            "Iteration 288, loss = 0.14366783\n",
            "Iteration 289, loss = 0.14347662\n",
            "Iteration 290, loss = 0.14326863\n",
            "Iteration 291, loss = 0.14307660\n",
            "Iteration 292, loss = 0.14288438\n",
            "Iteration 293, loss = 0.14267940\n",
            "Iteration 294, loss = 0.14244213\n",
            "Iteration 295, loss = 0.14225303\n",
            "Iteration 296, loss = 0.14206923\n",
            "Iteration 297, loss = 0.14186979\n",
            "Iteration 298, loss = 0.14164483\n",
            "Iteration 299, loss = 0.14144108\n",
            "Iteration 300, loss = 0.14123928\n",
            "Iteration 301, loss = 0.14105284\n",
            "Iteration 302, loss = 0.14084650\n",
            "Iteration 303, loss = 0.14066225\n",
            "Iteration 304, loss = 0.14048503\n",
            "Iteration 305, loss = 0.14024444\n",
            "Iteration 306, loss = 0.14001823\n",
            "Iteration 307, loss = 0.13982052\n",
            "Iteration 308, loss = 0.13964443\n",
            "Iteration 309, loss = 0.13942590\n",
            "Iteration 310, loss = 0.13923913\n",
            "Iteration 311, loss = 0.13902624\n",
            "Iteration 312, loss = 0.13881838\n",
            "Iteration 313, loss = 0.13863497\n",
            "Iteration 314, loss = 0.13843394\n",
            "Iteration 315, loss = 0.13823084\n",
            "Iteration 316, loss = 0.13800464\n",
            "Iteration 317, loss = 0.13783906\n",
            "Iteration 318, loss = 0.13762266\n",
            "Iteration 319, loss = 0.13745259\n",
            "Iteration 320, loss = 0.13716126\n",
            "Iteration 321, loss = 0.13693865\n",
            "Iteration 322, loss = 0.13675060\n",
            "Iteration 323, loss = 0.13655719\n",
            "Iteration 324, loss = 0.13633881\n",
            "Iteration 325, loss = 0.13610148\n",
            "Iteration 326, loss = 0.13589274\n",
            "Iteration 327, loss = 0.13566398\n",
            "Iteration 328, loss = 0.13543426\n",
            "Iteration 329, loss = 0.13519923\n",
            "Iteration 330, loss = 0.13498423\n",
            "Iteration 331, loss = 0.13472720\n",
            "Iteration 332, loss = 0.13449755\n",
            "Iteration 333, loss = 0.13428553\n",
            "Iteration 334, loss = 0.13403601\n",
            "Iteration 335, loss = 0.13381378\n",
            "Iteration 336, loss = 0.13357002\n",
            "Iteration 337, loss = 0.13335353\n",
            "Iteration 338, loss = 0.13313327\n",
            "Iteration 339, loss = 0.13289839\n",
            "Iteration 340, loss = 0.13268876\n",
            "Iteration 341, loss = 0.13242848\n",
            "Iteration 342, loss = 0.13218779\n",
            "Iteration 343, loss = 0.13197274\n",
            "Iteration 344, loss = 0.13168186\n",
            "Iteration 345, loss = 0.13150635\n",
            "Iteration 346, loss = 0.13124476\n",
            "Iteration 347, loss = 0.13096812\n",
            "Iteration 348, loss = 0.13071791\n",
            "Iteration 349, loss = 0.13047157\n",
            "Iteration 350, loss = 0.13019525\n",
            "Iteration 351, loss = 0.12992595\n",
            "Iteration 352, loss = 0.12965546\n",
            "Iteration 353, loss = 0.12936693\n",
            "Iteration 354, loss = 0.12909356\n",
            "Iteration 355, loss = 0.12879777\n",
            "Iteration 356, loss = 0.12852806\n",
            "Iteration 357, loss = 0.12824034\n",
            "Iteration 358, loss = 0.12806781\n",
            "Iteration 359, loss = 0.12770574\n",
            "Iteration 360, loss = 0.12742120\n",
            "Iteration 361, loss = 0.12716054\n",
            "Iteration 362, loss = 0.12689972\n",
            "Iteration 363, loss = 0.12666431\n",
            "Iteration 364, loss = 0.12640557\n",
            "Iteration 365, loss = 0.12629606\n",
            "Iteration 366, loss = 0.12584586\n",
            "Iteration 367, loss = 0.12559007\n",
            "Iteration 368, loss = 0.12537971\n",
            "Iteration 369, loss = 0.12511034\n",
            "Iteration 370, loss = 0.12484685\n",
            "Iteration 371, loss = 0.12461136\n",
            "Iteration 372, loss = 0.12430232\n",
            "Iteration 373, loss = 0.12407234\n",
            "Iteration 374, loss = 0.12386799\n",
            "Iteration 375, loss = 0.12350530\n",
            "Iteration 376, loss = 0.12325362\n",
            "Iteration 377, loss = 0.12298186\n",
            "Iteration 378, loss = 0.12273858\n",
            "Iteration 379, loss = 0.12249599\n",
            "Iteration 380, loss = 0.12221507\n",
            "Iteration 381, loss = 0.12197367\n",
            "Iteration 382, loss = 0.12175135\n",
            "Iteration 383, loss = 0.12150275\n",
            "Iteration 384, loss = 0.12122825\n",
            "Iteration 385, loss = 0.12094010\n",
            "Iteration 386, loss = 0.12070664\n",
            "Iteration 387, loss = 0.12045409\n",
            "Iteration 388, loss = 0.12018499\n",
            "Iteration 389, loss = 0.11994789\n",
            "Iteration 390, loss = 0.11965624\n",
            "Iteration 391, loss = 0.11941942\n",
            "Iteration 392, loss = 0.11915854\n",
            "Iteration 393, loss = 0.11886502\n",
            "Iteration 394, loss = 0.11857560\n",
            "Iteration 395, loss = 0.11832879\n",
            "Iteration 396, loss = 0.11803980\n",
            "Iteration 397, loss = 0.11780138\n",
            "Iteration 398, loss = 0.11751993\n",
            "Iteration 399, loss = 0.11728232\n",
            "Iteration 400, loss = 0.11699960\n",
            "Iteration 401, loss = 0.11679502\n",
            "Iteration 402, loss = 0.11646003\n",
            "Iteration 403, loss = 0.11620700\n",
            "Iteration 404, loss = 0.11594557\n",
            "Iteration 405, loss = 0.11567113\n",
            "Iteration 406, loss = 0.11531131\n",
            "Iteration 407, loss = 0.11508055\n",
            "Iteration 408, loss = 0.11468499\n",
            "Iteration 409, loss = 0.11439503\n",
            "Iteration 410, loss = 0.11404365\n",
            "Iteration 411, loss = 0.11371229\n",
            "Iteration 412, loss = 0.11343591\n",
            "Iteration 413, loss = 0.11306076\n",
            "Iteration 414, loss = 0.11275683\n",
            "Iteration 415, loss = 0.11240328\n",
            "Iteration 416, loss = 0.11211882\n",
            "Iteration 417, loss = 0.11187371\n",
            "Iteration 418, loss = 0.11148021\n",
            "Iteration 419, loss = 0.11120009\n",
            "Iteration 420, loss = 0.11088709\n",
            "Iteration 421, loss = 0.11066842\n",
            "Iteration 422, loss = 0.11031184\n",
            "Iteration 423, loss = 0.10999753\n",
            "Iteration 424, loss = 0.10966485\n",
            "Iteration 425, loss = 0.10942176\n",
            "Iteration 426, loss = 0.10907669\n",
            "Iteration 427, loss = 0.10877526\n",
            "Iteration 428, loss = 0.10843249\n",
            "Iteration 429, loss = 0.10814198\n",
            "Iteration 430, loss = 0.10780502\n",
            "Iteration 431, loss = 0.10754352\n",
            "Iteration 432, loss = 0.10728864\n",
            "Iteration 433, loss = 0.10691064\n",
            "Iteration 434, loss = 0.10659272\n",
            "Iteration 435, loss = 0.10622439\n",
            "Iteration 436, loss = 0.10593472\n",
            "Iteration 437, loss = 0.10557653\n",
            "Iteration 438, loss = 0.10520437\n",
            "Iteration 439, loss = 0.10488084\n",
            "Iteration 440, loss = 0.10451163\n",
            "Iteration 441, loss = 0.10417965\n",
            "Iteration 442, loss = 0.10377039\n",
            "Iteration 443, loss = 0.10342049\n",
            "Iteration 444, loss = 0.10304906\n",
            "Iteration 445, loss = 0.10272282\n",
            "Iteration 446, loss = 0.10238290\n",
            "Iteration 447, loss = 0.10199602\n",
            "Iteration 448, loss = 0.10164104\n",
            "Iteration 449, loss = 0.10134497\n",
            "Iteration 450, loss = 0.10092004\n",
            "Iteration 451, loss = 0.10056886\n",
            "Iteration 452, loss = 0.10019814\n",
            "Iteration 453, loss = 0.09985146\n",
            "Iteration 454, loss = 0.09959333\n",
            "Iteration 455, loss = 0.09913216\n",
            "Iteration 456, loss = 0.09878922\n",
            "Iteration 457, loss = 0.09848240\n",
            "Iteration 458, loss = 0.09810598\n",
            "Iteration 459, loss = 0.09785574\n",
            "Iteration 460, loss = 0.09744892\n",
            "Iteration 461, loss = 0.09713399\n",
            "Iteration 462, loss = 0.09678234\n",
            "Iteration 463, loss = 0.09639616\n",
            "Iteration 464, loss = 0.09603215\n",
            "Iteration 465, loss = 0.09566221\n",
            "Iteration 466, loss = 0.09526486\n",
            "Iteration 467, loss = 0.09485048\n",
            "Iteration 468, loss = 0.09456758\n",
            "Iteration 469, loss = 0.09417665\n",
            "Iteration 470, loss = 0.09380598\n",
            "Iteration 471, loss = 0.09348412\n",
            "Iteration 472, loss = 0.09301430\n",
            "Iteration 473, loss = 0.09265431\n",
            "Iteration 474, loss = 0.09227415\n",
            "Iteration 475, loss = 0.09194343\n",
            "Iteration 476, loss = 0.09155829\n",
            "Iteration 477, loss = 0.09116576\n",
            "Iteration 478, loss = 0.09081228\n",
            "Iteration 479, loss = 0.09044436\n",
            "Iteration 480, loss = 0.09006322\n",
            "Iteration 481, loss = 0.08969151\n",
            "Iteration 482, loss = 0.08933022\n",
            "Iteration 483, loss = 0.08894371\n",
            "Iteration 484, loss = 0.08860014\n",
            "Iteration 485, loss = 0.08814727\n",
            "Iteration 486, loss = 0.08775683\n",
            "Iteration 487, loss = 0.08734856\n",
            "Iteration 488, loss = 0.08696473\n",
            "Iteration 489, loss = 0.08656148\n",
            "Iteration 490, loss = 0.08626061\n",
            "Iteration 491, loss = 0.08590991\n",
            "Iteration 492, loss = 0.08546708\n",
            "Iteration 493, loss = 0.08514174\n",
            "Iteration 494, loss = 0.08473985\n",
            "Iteration 495, loss = 0.08442459\n",
            "Iteration 496, loss = 0.08406175\n",
            "Iteration 497, loss = 0.08369361\n",
            "Iteration 498, loss = 0.08335873\n",
            "Iteration 499, loss = 0.08308352\n",
            "Iteration 500, loss = 0.08274657\n",
            "Iteration 501, loss = 0.08243311\n",
            "Iteration 502, loss = 0.08211984\n",
            "Iteration 503, loss = 0.08180769\n",
            "Iteration 504, loss = 0.08153764\n",
            "Iteration 505, loss = 0.08117016\n",
            "Iteration 506, loss = 0.08091468\n",
            "Iteration 507, loss = 0.08069899\n",
            "Iteration 508, loss = 0.08037556\n",
            "Iteration 509, loss = 0.08002953\n",
            "Iteration 510, loss = 0.07971447\n",
            "Iteration 511, loss = 0.07943003\n",
            "Iteration 512, loss = 0.07912960\n",
            "Iteration 513, loss = 0.07894424\n",
            "Iteration 514, loss = 0.07861168\n",
            "Iteration 515, loss = 0.07828636\n",
            "Iteration 516, loss = 0.07801971\n",
            "Iteration 517, loss = 0.07769431\n",
            "Iteration 518, loss = 0.07743820\n",
            "Iteration 519, loss = 0.07714544\n",
            "Iteration 520, loss = 0.07687454\n",
            "Iteration 521, loss = 0.07658408\n",
            "Iteration 522, loss = 0.07632692\n",
            "Iteration 523, loss = 0.07605164\n",
            "Iteration 524, loss = 0.07576339\n",
            "Iteration 525, loss = 0.07561131\n",
            "Iteration 526, loss = 0.07528236\n",
            "Iteration 527, loss = 0.07504019\n",
            "Iteration 528, loss = 0.07477267\n",
            "Iteration 529, loss = 0.07452601\n",
            "Iteration 530, loss = 0.07429537\n",
            "Iteration 531, loss = 0.07411469\n",
            "Iteration 532, loss = 0.07382167\n",
            "Iteration 533, loss = 0.07353770\n",
            "Iteration 534, loss = 0.07335698\n",
            "Iteration 535, loss = 0.07307434\n",
            "Iteration 536, loss = 0.07285487\n",
            "Iteration 537, loss = 0.07258437\n",
            "Iteration 538, loss = 0.07237710\n",
            "Iteration 539, loss = 0.07210398\n",
            "Iteration 540, loss = 0.07189217\n",
            "Iteration 541, loss = 0.07164367\n",
            "Iteration 542, loss = 0.07141223\n",
            "Iteration 543, loss = 0.07120246\n",
            "Iteration 544, loss = 0.07094908\n",
            "Iteration 545, loss = 0.07072662\n",
            "Iteration 546, loss = 0.07050261\n",
            "Iteration 547, loss = 0.07028635\n",
            "Iteration 548, loss = 0.07002525\n",
            "Iteration 549, loss = 0.06977063\n",
            "Iteration 550, loss = 0.06957123\n",
            "Iteration 551, loss = 0.06934861\n",
            "Iteration 552, loss = 0.06910385\n",
            "Iteration 553, loss = 0.06889562\n",
            "Iteration 554, loss = 0.06865238\n",
            "Iteration 555, loss = 0.06843188\n",
            "Iteration 556, loss = 0.06820736\n",
            "Iteration 557, loss = 0.06801139\n",
            "Iteration 558, loss = 0.06776911\n",
            "Iteration 559, loss = 0.06753771\n",
            "Iteration 560, loss = 0.06729993\n",
            "Iteration 561, loss = 0.06709006\n",
            "Iteration 562, loss = 0.06689033\n",
            "Iteration 563, loss = 0.06666265\n",
            "Iteration 564, loss = 0.06642166\n",
            "Iteration 565, loss = 0.06626431\n",
            "Iteration 566, loss = 0.06603700\n",
            "Iteration 567, loss = 0.06576991\n",
            "Iteration 568, loss = 0.06568545\n",
            "Iteration 569, loss = 0.06548421\n",
            "Iteration 570, loss = 0.06514235\n",
            "Iteration 571, loss = 0.06496189\n",
            "Iteration 572, loss = 0.06472428\n",
            "Iteration 573, loss = 0.06449869\n",
            "Iteration 574, loss = 0.06431677\n",
            "Iteration 575, loss = 0.06410081\n",
            "Iteration 576, loss = 0.06388219\n",
            "Iteration 577, loss = 0.06367565\n",
            "Iteration 578, loss = 0.06349694\n",
            "Iteration 579, loss = 0.06328012\n",
            "Iteration 580, loss = 0.06308801\n",
            "Iteration 581, loss = 0.06287238\n",
            "Iteration 582, loss = 0.06265502\n",
            "Iteration 583, loss = 0.06250912\n",
            "Iteration 584, loss = 0.06231700\n",
            "Iteration 585, loss = 0.06206374\n",
            "Iteration 586, loss = 0.06186483\n",
            "Iteration 587, loss = 0.06165101\n",
            "Iteration 588, loss = 0.06144745\n",
            "Iteration 589, loss = 0.06123941\n",
            "Iteration 590, loss = 0.06103266\n",
            "Iteration 591, loss = 0.06086689\n",
            "Iteration 592, loss = 0.06067605\n",
            "Iteration 593, loss = 0.06049857\n",
            "Iteration 594, loss = 0.06028153\n",
            "Iteration 595, loss = 0.06007043\n",
            "Iteration 596, loss = 0.05990221\n",
            "Iteration 597, loss = 0.05973636\n",
            "Iteration 598, loss = 0.05951331\n",
            "Iteration 599, loss = 0.05927993\n",
            "Iteration 600, loss = 0.05911377\n",
            "Iteration 601, loss = 0.05890140\n",
            "Iteration 602, loss = 0.05872305\n",
            "Iteration 603, loss = 0.05854840\n",
            "Iteration 604, loss = 0.05836405\n",
            "Iteration 605, loss = 0.05815961\n",
            "Iteration 606, loss = 0.05795919\n",
            "Iteration 607, loss = 0.05778079\n",
            "Iteration 608, loss = 0.05760240\n",
            "Iteration 609, loss = 0.05739998\n",
            "Iteration 610, loss = 0.05723238\n",
            "Iteration 611, loss = 0.05704993\n",
            "Iteration 612, loss = 0.05686843\n",
            "Iteration 613, loss = 0.05669237\n",
            "Iteration 614, loss = 0.05651642\n",
            "Iteration 615, loss = 0.05639094\n",
            "Iteration 616, loss = 0.05617768\n",
            "Iteration 617, loss = 0.05599383\n",
            "Iteration 618, loss = 0.05578561\n",
            "Iteration 619, loss = 0.05558564\n",
            "Iteration 620, loss = 0.05540144\n",
            "Iteration 621, loss = 0.05522470\n",
            "Iteration 622, loss = 0.05507142\n",
            "Iteration 623, loss = 0.05491712\n",
            "Iteration 624, loss = 0.05470665\n",
            "Iteration 625, loss = 0.05452794\n",
            "Iteration 626, loss = 0.05435725\n",
            "Iteration 627, loss = 0.05419567\n",
            "Iteration 628, loss = 0.05404015\n",
            "Iteration 629, loss = 0.05383627\n",
            "Iteration 630, loss = 0.05366718\n",
            "Iteration 631, loss = 0.05352689\n",
            "Iteration 632, loss = 0.05337196\n",
            "Iteration 633, loss = 0.05317475\n",
            "Iteration 634, loss = 0.05301516\n",
            "Iteration 635, loss = 0.05283006\n",
            "Iteration 636, loss = 0.05266524\n",
            "Iteration 637, loss = 0.05250697\n",
            "Iteration 638, loss = 0.05234212\n",
            "Iteration 639, loss = 0.05219802\n",
            "Iteration 640, loss = 0.05199825\n",
            "Iteration 641, loss = 0.05183006\n",
            "Iteration 642, loss = 0.05167421\n",
            "Iteration 643, loss = 0.05153253\n",
            "Iteration 644, loss = 0.05137554\n",
            "Iteration 645, loss = 0.05119462\n",
            "Iteration 646, loss = 0.05109150\n",
            "Iteration 647, loss = 0.05088916\n",
            "Iteration 648, loss = 0.05075148\n",
            "Iteration 649, loss = 0.05055686\n",
            "Iteration 650, loss = 0.05044042\n",
            "Iteration 651, loss = 0.05026000\n",
            "Iteration 652, loss = 0.05012081\n",
            "Iteration 653, loss = 0.04992151\n",
            "Iteration 654, loss = 0.04980640\n",
            "Iteration 655, loss = 0.04965568\n",
            "Iteration 656, loss = 0.04955702\n",
            "Iteration 657, loss = 0.04934553\n",
            "Iteration 658, loss = 0.04923250\n",
            "Iteration 659, loss = 0.04900315\n",
            "Iteration 660, loss = 0.04887166\n",
            "Iteration 661, loss = 0.04874992\n",
            "Iteration 662, loss = 0.04858036\n",
            "Iteration 663, loss = 0.04844130\n",
            "Iteration 664, loss = 0.04829856\n",
            "Iteration 665, loss = 0.04814485\n",
            "Iteration 666, loss = 0.04803564\n",
            "Iteration 667, loss = 0.04784778\n",
            "Iteration 668, loss = 0.04766514\n",
            "Iteration 669, loss = 0.04759376\n",
            "Iteration 670, loss = 0.04742027\n",
            "Iteration 671, loss = 0.04729943\n",
            "Iteration 672, loss = 0.04708941\n",
            "Iteration 673, loss = 0.04695655\n",
            "Iteration 674, loss = 0.04681399\n",
            "Iteration 675, loss = 0.04665563\n",
            "Iteration 676, loss = 0.04654932\n",
            "Iteration 677, loss = 0.04638226\n",
            "Iteration 678, loss = 0.04629045\n",
            "Iteration 679, loss = 0.04611709\n",
            "Iteration 680, loss = 0.04598721\n",
            "Iteration 681, loss = 0.04583530\n",
            "Iteration 682, loss = 0.04568448\n",
            "Iteration 683, loss = 0.04559155\n",
            "Iteration 684, loss = 0.04548415\n",
            "Iteration 685, loss = 0.04529759\n",
            "Iteration 686, loss = 0.04520227\n",
            "Iteration 687, loss = 0.04501544\n",
            "Iteration 688, loss = 0.04487381\n",
            "Iteration 689, loss = 0.04473276\n",
            "Iteration 690, loss = 0.04460997\n",
            "Iteration 691, loss = 0.04458275\n",
            "Iteration 692, loss = 0.04439741\n",
            "Iteration 693, loss = 0.04418526\n",
            "Iteration 694, loss = 0.04414188\n",
            "Iteration 695, loss = 0.04397107\n",
            "Iteration 696, loss = 0.04381471\n",
            "Iteration 697, loss = 0.04369119\n",
            "Iteration 698, loss = 0.04353329\n",
            "Iteration 699, loss = 0.04339789\n",
            "Iteration 700, loss = 0.04327691\n",
            "Iteration 701, loss = 0.04315577\n",
            "Iteration 702, loss = 0.04300339\n",
            "Iteration 703, loss = 0.04298289\n",
            "Iteration 704, loss = 0.04279840\n",
            "Iteration 705, loss = 0.04265983\n",
            "Iteration 706, loss = 0.04253496\n",
            "Iteration 707, loss = 0.04237163\n",
            "Iteration 708, loss = 0.04224319\n",
            "Iteration 709, loss = 0.04213590\n",
            "Iteration 710, loss = 0.04199921\n",
            "Iteration 711, loss = 0.04191508\n",
            "Iteration 712, loss = 0.04176940\n",
            "Iteration 713, loss = 0.04164175\n",
            "Iteration 714, loss = 0.04151270\n",
            "Iteration 715, loss = 0.04149562\n",
            "Iteration 716, loss = 0.04126413\n",
            "Iteration 717, loss = 0.04113538\n",
            "Iteration 718, loss = 0.04101649\n",
            "Iteration 719, loss = 0.04089947\n",
            "Iteration 720, loss = 0.04080404\n",
            "Iteration 721, loss = 0.04067543\n",
            "Iteration 722, loss = 0.04054247\n",
            "Iteration 723, loss = 0.04040563\n",
            "Iteration 724, loss = 0.04029079\n",
            "Iteration 725, loss = 0.04017835\n",
            "Iteration 726, loss = 0.04004788\n",
            "Iteration 727, loss = 0.03995802\n",
            "Iteration 728, loss = 0.03986193\n",
            "Iteration 729, loss = 0.03970955\n",
            "Iteration 730, loss = 0.03959148\n",
            "Iteration 731, loss = 0.03945300\n",
            "Iteration 732, loss = 0.03941698\n",
            "Iteration 733, loss = 0.03926222\n",
            "Iteration 734, loss = 0.03922030\n",
            "Iteration 735, loss = 0.03904211\n",
            "Iteration 736, loss = 0.03892504\n",
            "Iteration 737, loss = 0.03880002\n",
            "Iteration 738, loss = 0.03867905\n",
            "Iteration 739, loss = 0.03857908\n",
            "Iteration 740, loss = 0.03855671\n",
            "Iteration 741, loss = 0.03835818\n",
            "Iteration 742, loss = 0.03826979\n",
            "Iteration 743, loss = 0.03814116\n",
            "Iteration 744, loss = 0.03805920\n",
            "Iteration 745, loss = 0.03792415\n",
            "Iteration 746, loss = 0.03785014\n",
            "Iteration 747, loss = 0.03770012\n",
            "Iteration 748, loss = 0.03762085\n",
            "Iteration 749, loss = 0.03750562\n",
            "Iteration 750, loss = 0.03739828\n",
            "Iteration 751, loss = 0.03729772\n",
            "Iteration 752, loss = 0.03718486\n",
            "Iteration 753, loss = 0.03705984\n",
            "Iteration 754, loss = 0.03704164\n",
            "Iteration 755, loss = 0.03689015\n",
            "Iteration 756, loss = 0.03670856\n",
            "Iteration 757, loss = 0.03663548\n",
            "Iteration 758, loss = 0.03652436\n",
            "Iteration 759, loss = 0.03647860\n",
            "Iteration 760, loss = 0.03632244\n",
            "Iteration 761, loss = 0.03621941\n",
            "Iteration 762, loss = 0.03612711\n",
            "Iteration 763, loss = 0.03604872\n",
            "Iteration 764, loss = 0.03592790\n",
            "Iteration 765, loss = 0.03584723\n",
            "Iteration 766, loss = 0.03570764\n",
            "Iteration 767, loss = 0.03561296\n",
            "Iteration 768, loss = 0.03551622\n",
            "Iteration 769, loss = 0.03544305\n",
            "Iteration 770, loss = 0.03530044\n",
            "Iteration 771, loss = 0.03522425\n",
            "Iteration 772, loss = 0.03512179\n",
            "Iteration 773, loss = 0.03500390\n",
            "Iteration 774, loss = 0.03491393\n",
            "Iteration 775, loss = 0.03483329\n",
            "Iteration 776, loss = 0.03472644\n",
            "Iteration 777, loss = 0.03464549\n",
            "Iteration 778, loss = 0.03451187\n",
            "Iteration 779, loss = 0.03441233\n",
            "Iteration 780, loss = 0.03438245\n",
            "Iteration 781, loss = 0.03425255\n",
            "Iteration 782, loss = 0.03420406\n",
            "Iteration 783, loss = 0.03404526\n",
            "Iteration 784, loss = 0.03392623\n",
            "Iteration 785, loss = 0.03388351\n",
            "Iteration 786, loss = 0.03379533\n",
            "Iteration 787, loss = 0.03365324\n",
            "Iteration 788, loss = 0.03356804\n",
            "Iteration 789, loss = 0.03345999\n",
            "Iteration 790, loss = 0.03339235\n",
            "Iteration 791, loss = 0.03329424\n",
            "Iteration 792, loss = 0.03319003\n",
            "Iteration 793, loss = 0.03316824\n",
            "Iteration 794, loss = 0.03313699\n",
            "Iteration 795, loss = 0.03291558\n",
            "Iteration 796, loss = 0.03284009\n",
            "Iteration 797, loss = 0.03275870\n",
            "Iteration 798, loss = 0.03266321\n",
            "Iteration 799, loss = 0.03254899\n",
            "Iteration 800, loss = 0.03249010\n",
            "Iteration 801, loss = 0.03237979\n",
            "Iteration 802, loss = 0.03232828\n",
            "Iteration 803, loss = 0.03221896\n",
            "Iteration 804, loss = 0.03219824\n",
            "Iteration 805, loss = 0.03212910\n",
            "Iteration 806, loss = 0.03197731\n",
            "Iteration 807, loss = 0.03188863\n",
            "Iteration 808, loss = 0.03177751\n",
            "Iteration 809, loss = 0.03172172\n",
            "Iteration 810, loss = 0.03161617\n",
            "Iteration 811, loss = 0.03157770\n",
            "Iteration 812, loss = 0.03145348\n",
            "Iteration 813, loss = 0.03139799\n",
            "Iteration 814, loss = 0.03127507\n",
            "Iteration 815, loss = 0.03120302\n",
            "Iteration 816, loss = 0.03107700\n",
            "Iteration 817, loss = 0.03106881\n",
            "Iteration 818, loss = 0.03095815\n",
            "Iteration 819, loss = 0.03084176\n",
            "Iteration 820, loss = 0.03078556\n",
            "Iteration 821, loss = 0.03068716\n",
            "Iteration 822, loss = 0.03064557\n",
            "Iteration 823, loss = 0.03053452\n",
            "Iteration 824, loss = 0.03042918\n",
            "Iteration 825, loss = 0.03035961\n",
            "Iteration 826, loss = 0.03028674\n",
            "Iteration 827, loss = 0.03017039\n",
            "Iteration 828, loss = 0.03012774\n",
            "Iteration 829, loss = 0.03007468\n",
            "Iteration 830, loss = 0.02998005\n",
            "Iteration 831, loss = 0.02986766\n",
            "Iteration 832, loss = 0.02977914\n",
            "Iteration 833, loss = 0.02972386\n",
            "Iteration 834, loss = 0.02964439\n",
            "Iteration 835, loss = 0.02956168\n",
            "Iteration 836, loss = 0.02950159\n",
            "Iteration 837, loss = 0.02940223\n",
            "Iteration 838, loss = 0.02933425\n",
            "Iteration 839, loss = 0.02932845\n",
            "Iteration 840, loss = 0.02919315\n",
            "Iteration 841, loss = 0.02907687\n",
            "Iteration 842, loss = 0.02905162\n",
            "Iteration 843, loss = 0.02899171\n",
            "Iteration 844, loss = 0.02886770\n",
            "Iteration 845, loss = 0.02881473\n",
            "Iteration 846, loss = 0.02879224\n",
            "Iteration 847, loss = 0.02870869\n",
            "Iteration 848, loss = 0.02854853\n",
            "Iteration 849, loss = 0.02850906\n",
            "Iteration 850, loss = 0.02842800\n",
            "Iteration 851, loss = 0.02837285\n",
            "Iteration 852, loss = 0.02825060\n",
            "Iteration 853, loss = 0.02824393\n",
            "Iteration 854, loss = 0.02815420\n",
            "Iteration 855, loss = 0.02805603\n",
            "Iteration 856, loss = 0.02798437\n",
            "Iteration 857, loss = 0.02793581\n",
            "Iteration 858, loss = 0.02786257\n",
            "Iteration 859, loss = 0.02780279\n",
            "Iteration 860, loss = 0.02766804\n",
            "Iteration 861, loss = 0.02779153\n",
            "Iteration 862, loss = 0.02758605\n",
            "Iteration 863, loss = 0.02744768\n",
            "Iteration 864, loss = 0.02740533\n",
            "Iteration 865, loss = 0.02734400\n",
            "Iteration 866, loss = 0.02727807\n",
            "Iteration 867, loss = 0.02722374\n",
            "Iteration 868, loss = 0.02712242\n",
            "Iteration 869, loss = 0.02704051\n",
            "Iteration 870, loss = 0.02700524\n",
            "Iteration 871, loss = 0.02690170\n",
            "Iteration 872, loss = 0.02684455\n",
            "Iteration 873, loss = 0.02674786\n",
            "Iteration 874, loss = 0.02669577\n",
            "Iteration 875, loss = 0.02662491\n",
            "Iteration 876, loss = 0.02657510\n",
            "Iteration 877, loss = 0.02655945\n",
            "Iteration 878, loss = 0.02643354\n",
            "Iteration 879, loss = 0.02634560\n",
            "Iteration 880, loss = 0.02630316\n",
            "Iteration 881, loss = 0.02621195\n",
            "Iteration 882, loss = 0.02614996\n",
            "Iteration 883, loss = 0.02610722\n",
            "Iteration 884, loss = 0.02603069\n",
            "Iteration 885, loss = 0.02598464\n",
            "Iteration 886, loss = 0.02591274\n",
            "Iteration 887, loss = 0.02583693\n",
            "Iteration 888, loss = 0.02577395\n",
            "Iteration 889, loss = 0.02571410\n",
            "Iteration 890, loss = 0.02564978\n",
            "Iteration 891, loss = 0.02559117\n",
            "Iteration 892, loss = 0.02552169\n",
            "Iteration 893, loss = 0.02557216\n",
            "Iteration 894, loss = 0.02537049\n",
            "Iteration 895, loss = 0.02532389\n",
            "Iteration 896, loss = 0.02525196\n",
            "Iteration 897, loss = 0.02516915\n",
            "Iteration 898, loss = 0.02514197\n",
            "Iteration 899, loss = 0.02510024\n",
            "Iteration 900, loss = 0.02499430\n",
            "Iteration 901, loss = 0.02492080\n",
            "Iteration 902, loss = 0.02491378\n",
            "Iteration 903, loss = 0.02478958\n",
            "Iteration 904, loss = 0.02476686\n",
            "Iteration 905, loss = 0.02472236\n",
            "Iteration 906, loss = 0.02463432\n",
            "Iteration 907, loss = 0.02455251\n",
            "Iteration 908, loss = 0.02452231\n",
            "Iteration 909, loss = 0.02448231\n",
            "Iteration 910, loss = 0.02451568\n",
            "Iteration 911, loss = 0.02431838\n",
            "Iteration 912, loss = 0.02428855\n",
            "Iteration 913, loss = 0.02421720\n",
            "Iteration 914, loss = 0.02416909\n",
            "Iteration 915, loss = 0.02413200\n",
            "Iteration 916, loss = 0.02403460\n",
            "Iteration 917, loss = 0.02394454\n",
            "Iteration 918, loss = 0.02389197\n",
            "Iteration 919, loss = 0.02385372\n",
            "Iteration 920, loss = 0.02381883\n",
            "Iteration 921, loss = 0.02372129\n",
            "Iteration 922, loss = 0.02371103\n",
            "Iteration 923, loss = 0.02362439\n",
            "Iteration 924, loss = 0.02354129\n",
            "Iteration 925, loss = 0.02350468\n",
            "Iteration 926, loss = 0.02342967\n",
            "Iteration 927, loss = 0.02337990\n",
            "Iteration 928, loss = 0.02342318\n",
            "Iteration 929, loss = 0.02326619\n",
            "Iteration 930, loss = 0.02322442\n",
            "Iteration 931, loss = 0.02315340\n",
            "Iteration 932, loss = 0.02310715\n",
            "Iteration 933, loss = 0.02304602\n",
            "Iteration 934, loss = 0.02298616\n",
            "Iteration 935, loss = 0.02296334\n",
            "Iteration 936, loss = 0.02290586\n",
            "Iteration 937, loss = 0.02283606\n",
            "Iteration 938, loss = 0.02279119\n",
            "Iteration 939, loss = 0.02269813\n",
            "Iteration 940, loss = 0.02266403\n",
            "Iteration 941, loss = 0.02258703\n",
            "Iteration 942, loss = 0.02253629\n",
            "Iteration 943, loss = 0.02254483\n",
            "Iteration 944, loss = 0.02245859\n",
            "Iteration 945, loss = 0.02236653\n",
            "Iteration 946, loss = 0.02236719\n",
            "Iteration 947, loss = 0.02231770\n",
            "Iteration 948, loss = 0.02224876\n",
            "Iteration 949, loss = 0.02217058\n",
            "Iteration 950, loss = 0.02212994\n",
            "Iteration 951, loss = 0.02207934\n",
            "Iteration 952, loss = 0.02200395\n",
            "Iteration 953, loss = 0.02195740\n",
            "Iteration 954, loss = 0.02192361\n",
            "Iteration 955, loss = 0.02189206\n",
            "Iteration 956, loss = 0.02185485\n",
            "Iteration 957, loss = 0.02173926\n",
            "Iteration 958, loss = 0.02169798\n",
            "Iteration 959, loss = 0.02165274\n",
            "Iteration 960, loss = 0.02163068\n",
            "Iteration 961, loss = 0.02152125\n",
            "Iteration 962, loss = 0.02150104\n",
            "Iteration 963, loss = 0.02144473\n",
            "Iteration 964, loss = 0.02139257\n",
            "Iteration 965, loss = 0.02135230\n",
            "Iteration 966, loss = 0.02136878\n",
            "Iteration 967, loss = 0.02123819\n",
            "Iteration 968, loss = 0.02119183\n",
            "Iteration 969, loss = 0.02114079\n",
            "Iteration 970, loss = 0.02111548\n",
            "Iteration 971, loss = 0.02105595\n",
            "Iteration 972, loss = 0.02099677\n",
            "Iteration 973, loss = 0.02095164\n",
            "Iteration 974, loss = 0.02091031\n",
            "Iteration 975, loss = 0.02088513\n",
            "Iteration 976, loss = 0.02077235\n",
            "Iteration 977, loss = 0.02074405\n",
            "Iteration 978, loss = 0.02071423\n",
            "Iteration 979, loss = 0.02071976\n",
            "Iteration 980, loss = 0.02063152\n",
            "Iteration 981, loss = 0.02060283\n",
            "Iteration 982, loss = 0.02056193\n",
            "Iteration 983, loss = 0.02045900\n",
            "Iteration 984, loss = 0.02044467\n",
            "Iteration 985, loss = 0.02036324\n",
            "Iteration 986, loss = 0.02030427\n",
            "Iteration 987, loss = 0.02025517\n",
            "Iteration 988, loss = 0.02020961\n",
            "Iteration 989, loss = 0.02019048\n",
            "Iteration 990, loss = 0.02012626\n",
            "Iteration 991, loss = 0.02005323\n",
            "Iteration 992, loss = 0.02009583\n",
            "Iteration 993, loss = 0.02004749\n",
            "Iteration 994, loss = 0.01995420\n",
            "Iteration 995, loss = 0.01993425\n",
            "Iteration 996, loss = 0.01986472\n",
            "Iteration 997, loss = 0.01981967\n",
            "Iteration 998, loss = 0.01979064\n",
            "Iteration 999, loss = 0.01972649\n",
            "Iteration 1000, loss = 0.01970667\n",
            "Iteration 1001, loss = 0.01963864\n",
            "Iteration 1002, loss = 0.01959878\n",
            "Iteration 1003, loss = 0.01953604\n",
            "Iteration 1004, loss = 0.01949981\n",
            "Iteration 1005, loss = 0.01945589\n",
            "Iteration 1006, loss = 0.01943493\n",
            "Iteration 1007, loss = 0.01937188\n",
            "Iteration 1008, loss = 0.01930479\n",
            "Iteration 1009, loss = 0.01927177\n",
            "Iteration 1010, loss = 0.01920912\n",
            "Iteration 1011, loss = 0.01919686\n",
            "Iteration 1012, loss = 0.01910553\n",
            "Iteration 1013, loss = 0.01908763\n",
            "Iteration 1014, loss = 0.01907250\n",
            "Iteration 1015, loss = 0.01902880\n",
            "Iteration 1016, loss = 0.01896373\n",
            "Iteration 1017, loss = 0.01890410\n",
            "Iteration 1018, loss = 0.01890708\n",
            "Iteration 1019, loss = 0.01884244\n",
            "Iteration 1020, loss = 0.01878683\n",
            "Iteration 1021, loss = 0.01875451\n",
            "Iteration 1022, loss = 0.01869153\n",
            "Iteration 1023, loss = 0.01867246\n",
            "Iteration 1024, loss = 0.01862392\n",
            "Iteration 1025, loss = 0.01858361\n",
            "Iteration 1026, loss = 0.01852436\n",
            "Iteration 1027, loss = 0.01852150\n",
            "Iteration 1028, loss = 0.01854200\n",
            "Iteration 1029, loss = 0.01843780\n",
            "Iteration 1030, loss = 0.01840945\n",
            "Iteration 1031, loss = 0.01832269\n",
            "Iteration 1032, loss = 0.01830174\n",
            "Iteration 1033, loss = 0.01828035\n",
            "Iteration 1034, loss = 0.01819942\n",
            "Iteration 1035, loss = 0.01818084\n",
            "Iteration 1036, loss = 0.01815964\n",
            "Iteration 1037, loss = 0.01807840\n",
            "Iteration 1038, loss = 0.01805835\n",
            "Iteration 1039, loss = 0.01803611\n",
            "Iteration 1040, loss = 0.01795783\n",
            "Iteration 1041, loss = 0.01791614\n",
            "Iteration 1042, loss = 0.01788990\n",
            "Iteration 1043, loss = 0.01786112\n",
            "Iteration 1044, loss = 0.01780952\n",
            "Iteration 1045, loss = 0.01777163\n",
            "Iteration 1046, loss = 0.01774989\n",
            "Iteration 1047, loss = 0.01770558\n",
            "Iteration 1048, loss = 0.01764609\n",
            "Iteration 1049, loss = 0.01762656\n",
            "Iteration 1050, loss = 0.01757068\n",
            "Iteration 1051, loss = 0.01757514\n",
            "Iteration 1052, loss = 0.01751103\n",
            "Iteration 1053, loss = 0.01745137\n",
            "Iteration 1054, loss = 0.01743569\n",
            "Iteration 1055, loss = 0.01738998\n",
            "Iteration 1056, loss = 0.01734596\n",
            "Iteration 1057, loss = 0.01731718\n",
            "Iteration 1058, loss = 0.01726923\n",
            "Iteration 1059, loss = 0.01722418\n",
            "Iteration 1060, loss = 0.01721851\n",
            "Iteration 1061, loss = 0.01718836\n",
            "Iteration 1062, loss = 0.01711548\n",
            "Iteration 1063, loss = 0.01709059\n",
            "Iteration 1064, loss = 0.01708467\n",
            "Iteration 1065, loss = 0.01699592\n",
            "Iteration 1066, loss = 0.01697031\n",
            "Iteration 1067, loss = 0.01693292\n",
            "Iteration 1068, loss = 0.01692462\n",
            "Iteration 1069, loss = 0.01689301\n",
            "Iteration 1070, loss = 0.01685852\n",
            "Iteration 1071, loss = 0.01685580\n",
            "Iteration 1072, loss = 0.01679348\n",
            "Iteration 1073, loss = 0.01672193\n",
            "Iteration 1074, loss = 0.01672010\n",
            "Iteration 1075, loss = 0.01662341\n",
            "Iteration 1076, loss = 0.01662181\n",
            "Iteration 1077, loss = 0.01659324\n",
            "Iteration 1078, loss = 0.01652737\n",
            "Iteration 1079, loss = 0.01646092\n",
            "Iteration 1080, loss = 0.01651791\n",
            "Iteration 1081, loss = 0.01648101\n",
            "Iteration 1082, loss = 0.01639620\n",
            "Iteration 1083, loss = 0.01638874\n",
            "Iteration 1084, loss = 0.01631925\n",
            "Iteration 1085, loss = 0.01630605\n",
            "Iteration 1086, loss = 0.01628192\n",
            "Iteration 1087, loss = 0.01621688\n",
            "Iteration 1088, loss = 0.01623611\n",
            "Iteration 1089, loss = 0.01614238\n",
            "Iteration 1090, loss = 0.01614308\n",
            "Iteration 1091, loss = 0.01613056\n",
            "Iteration 1092, loss = 0.01604572\n",
            "Iteration 1093, loss = 0.01600198\n",
            "Iteration 1094, loss = 0.01599211\n",
            "Iteration 1095, loss = 0.01597363\n",
            "Iteration 1096, loss = 0.01590889\n",
            "Iteration 1097, loss = 0.01588304\n",
            "Iteration 1098, loss = 0.01585676\n",
            "Iteration 1099, loss = 0.01580901\n",
            "Iteration 1100, loss = 0.01579720\n",
            "Iteration 1101, loss = 0.01576598\n",
            "Iteration 1102, loss = 0.01573622\n",
            "Iteration 1103, loss = 0.01579337\n",
            "Iteration 1104, loss = 0.01570249\n",
            "Iteration 1105, loss = 0.01561313\n",
            "Iteration 1106, loss = 0.01564384\n",
            "Iteration 1107, loss = 0.01555769\n",
            "Iteration 1108, loss = 0.01552634\n",
            "Iteration 1109, loss = 0.01554852\n",
            "Iteration 1110, loss = 0.01554428\n",
            "Iteration 1111, loss = 0.01548746\n",
            "Iteration 1112, loss = 0.01541225\n",
            "Iteration 1113, loss = 0.01537141\n",
            "Iteration 1114, loss = 0.01534493\n",
            "Iteration 1115, loss = 0.01532363\n",
            "Iteration 1116, loss = 0.01526698\n",
            "Iteration 1117, loss = 0.01523411\n",
            "Iteration 1118, loss = 0.01520909\n",
            "Iteration 1119, loss = 0.01517174\n",
            "Iteration 1120, loss = 0.01515945\n",
            "Iteration 1121, loss = 0.01511351\n",
            "Iteration 1122, loss = 0.01514792\n",
            "Iteration 1123, loss = 0.01504441\n",
            "Iteration 1124, loss = 0.01504109\n",
            "Iteration 1125, loss = 0.01503363\n",
            "Iteration 1126, loss = 0.01500217\n",
            "Iteration 1127, loss = 0.01493758\n",
            "Iteration 1128, loss = 0.01490215\n",
            "Iteration 1129, loss = 0.01488294\n",
            "Iteration 1130, loss = 0.01483907\n",
            "Iteration 1131, loss = 0.01481550\n",
            "Iteration 1132, loss = 0.01481598\n",
            "Iteration 1133, loss = 0.01476151\n",
            "Iteration 1134, loss = 0.01471110\n",
            "Iteration 1135, loss = 0.01470971\n",
            "Iteration 1136, loss = 0.01466066\n",
            "Iteration 1137, loss = 0.01464479\n",
            "Iteration 1138, loss = 0.01485657\n",
            "Iteration 1139, loss = 0.01462697\n",
            "Iteration 1140, loss = 0.01455804\n",
            "Iteration 1141, loss = 0.01451351\n",
            "Iteration 1142, loss = 0.01452036\n",
            "Iteration 1143, loss = 0.01454017\n",
            "Iteration 1144, loss = 0.01442803\n",
            "Iteration 1145, loss = 0.01441248\n",
            "Iteration 1146, loss = 0.01437275\n",
            "Iteration 1147, loss = 0.01436173\n",
            "Iteration 1148, loss = 0.01435206\n",
            "Iteration 1149, loss = 0.01426901\n",
            "Iteration 1150, loss = 0.01425219\n",
            "Iteration 1151, loss = 0.01424382\n",
            "Iteration 1152, loss = 0.01423210\n",
            "Iteration 1153, loss = 0.01415577\n",
            "Iteration 1154, loss = 0.01414590\n",
            "Iteration 1155, loss = 0.01412814\n",
            "Iteration 1156, loss = 0.01412864\n",
            "Iteration 1157, loss = 0.01411208\n",
            "Iteration 1158, loss = 0.01402684\n",
            "Iteration 1159, loss = 0.01399866\n",
            "Iteration 1160, loss = 0.01396439\n",
            "Iteration 1161, loss = 0.01396147\n",
            "Iteration 1162, loss = 0.01394273\n",
            "Iteration 1163, loss = 0.01392128\n",
            "Iteration 1164, loss = 0.01385861\n",
            "Iteration 1165, loss = 0.01384828\n",
            "Iteration 1166, loss = 0.01382204\n",
            "Iteration 1167, loss = 0.01383616\n",
            "Iteration 1168, loss = 0.01376176\n",
            "Iteration 1169, loss = 0.01378108\n",
            "Iteration 1170, loss = 0.01369332\n",
            "Iteration 1171, loss = 0.01379578\n",
            "Iteration 1172, loss = 0.01366863\n",
            "Iteration 1173, loss = 0.01360964\n",
            "Iteration 1174, loss = 0.01363544\n",
            "Iteration 1175, loss = 0.01358210\n",
            "Iteration 1176, loss = 0.01355244\n",
            "Iteration 1177, loss = 0.01351908\n",
            "Iteration 1178, loss = 0.01348778\n",
            "Iteration 1179, loss = 0.01347105\n",
            "Iteration 1180, loss = 0.01346368\n",
            "Iteration 1181, loss = 0.01338803\n",
            "Iteration 1182, loss = 0.01338331\n",
            "Iteration 1183, loss = 0.01338417\n",
            "Iteration 1184, loss = 0.01337318\n",
            "Iteration 1185, loss = 0.01331180\n",
            "Iteration 1186, loss = 0.01328676\n",
            "Iteration 1187, loss = 0.01325448\n",
            "Iteration 1188, loss = 0.01325341\n",
            "Iteration 1189, loss = 0.01320316\n",
            "Iteration 1190, loss = 0.01318417\n",
            "Iteration 1191, loss = 0.01317563\n",
            "Iteration 1192, loss = 0.01313305\n",
            "Iteration 1193, loss = 0.01308305\n",
            "Iteration 1194, loss = 0.01312410\n",
            "Iteration 1195, loss = 0.01309159\n",
            "Iteration 1196, loss = 0.01305444\n",
            "Iteration 1197, loss = 0.01301322\n",
            "Iteration 1198, loss = 0.01305154\n",
            "Iteration 1199, loss = 0.01297267\n",
            "Iteration 1200, loss = 0.01295236\n",
            "Iteration 1201, loss = 0.01292840\n",
            "Iteration 1202, loss = 0.01288535\n",
            "Iteration 1203, loss = 0.01291269\n",
            "Iteration 1204, loss = 0.01286717\n",
            "Iteration 1205, loss = 0.01281320\n",
            "Iteration 1206, loss = 0.01279332\n",
            "Iteration 1207, loss = 0.01275481\n",
            "Iteration 1208, loss = 0.01274175\n",
            "Iteration 1209, loss = 0.01273036\n",
            "Iteration 1210, loss = 0.01269306\n",
            "Iteration 1211, loss = 0.01266988\n",
            "Iteration 1212, loss = 0.01263469\n",
            "Iteration 1213, loss = 0.01264153\n",
            "Iteration 1214, loss = 0.01261482\n",
            "Iteration 1215, loss = 0.01260702\n",
            "Iteration 1216, loss = 0.01252471\n",
            "Iteration 1217, loss = 0.01255370\n",
            "Iteration 1218, loss = 0.01251180\n",
            "Iteration 1219, loss = 0.01250948\n",
            "Iteration 1220, loss = 0.01246983\n",
            "Iteration 1221, loss = 0.01244208\n",
            "Iteration 1222, loss = 0.01242833\n",
            "Iteration 1223, loss = 0.01238445\n",
            "Iteration 1224, loss = 0.01235880\n",
            "Iteration 1225, loss = 0.01233240\n",
            "Iteration 1226, loss = 0.01230402\n",
            "Iteration 1227, loss = 0.01230306\n",
            "Iteration 1228, loss = 0.01227202\n",
            "Iteration 1229, loss = 0.01225386\n",
            "Iteration 1230, loss = 0.01222694\n",
            "Iteration 1231, loss = 0.01221665\n",
            "Iteration 1232, loss = 0.01219794\n",
            "Iteration 1233, loss = 0.01225189\n",
            "Iteration 1234, loss = 0.01219528\n",
            "Iteration 1235, loss = 0.01215744\n",
            "Iteration 1236, loss = 0.01206665\n",
            "Iteration 1237, loss = 0.01209753\n",
            "Iteration 1238, loss = 0.01207229\n",
            "Iteration 1239, loss = 0.01203832\n",
            "Iteration 1240, loss = 0.01198911\n",
            "Iteration 1241, loss = 0.01196667\n",
            "Iteration 1242, loss = 0.01194533\n",
            "Iteration 1243, loss = 0.01193170\n",
            "Iteration 1244, loss = 0.01192643\n",
            "Iteration 1245, loss = 0.01190731\n",
            "Iteration 1246, loss = 0.01185868\n",
            "Iteration 1247, loss = 0.01186371\n",
            "Iteration 1248, loss = 0.01183710\n",
            "Iteration 1249, loss = 0.01183710\n",
            "Iteration 1250, loss = 0.01189859\n",
            "Iteration 1251, loss = 0.01185796\n",
            "Iteration 1252, loss = 0.01175347\n",
            "Iteration 1253, loss = 0.01174837\n",
            "Iteration 1254, loss = 0.01169441\n",
            "Iteration 1255, loss = 0.01170718\n",
            "Iteration 1256, loss = 0.01164831\n",
            "Iteration 1257, loss = 0.01166109\n",
            "Iteration 1258, loss = 0.01163051\n",
            "Iteration 1259, loss = 0.01161602\n",
            "Iteration 1260, loss = 0.01158781\n",
            "Iteration 1261, loss = 0.01162140\n",
            "Iteration 1262, loss = 0.01162467\n",
            "Iteration 1263, loss = 0.01159840\n",
            "Iteration 1264, loss = 0.01149121\n",
            "Iteration 1265, loss = 0.01146071\n",
            "Iteration 1266, loss = 0.01147816\n",
            "Iteration 1267, loss = 0.01141715\n",
            "Iteration 1268, loss = 0.01142182\n",
            "Iteration 1269, loss = 0.01141858\n",
            "Iteration 1270, loss = 0.01138319\n",
            "Iteration 1271, loss = 0.01133729\n",
            "Iteration 1272, loss = 0.01132324\n",
            "Iteration 1273, loss = 0.01132536\n",
            "Iteration 1274, loss = 0.01126638\n",
            "Iteration 1275, loss = 0.01127388\n",
            "Iteration 1276, loss = 0.01126468\n",
            "Iteration 1277, loss = 0.01121895\n",
            "Iteration 1278, loss = 0.01122676\n",
            "Iteration 1279, loss = 0.01118198\n",
            "Iteration 1280, loss = 0.01117691\n",
            "Iteration 1281, loss = 0.01113987\n",
            "Iteration 1282, loss = 0.01114772\n",
            "Iteration 1283, loss = 0.01111983\n",
            "Iteration 1284, loss = 0.01106655\n",
            "Iteration 1285, loss = 0.01115056\n",
            "Iteration 1286, loss = 0.01111929\n",
            "Iteration 1287, loss = 0.01102840\n",
            "Iteration 1288, loss = 0.01105748\n",
            "Iteration 1289, loss = 0.01100925\n",
            "Iteration 1290, loss = 0.01096882\n",
            "Iteration 1291, loss = 0.01098160\n",
            "Iteration 1292, loss = 0.01099904\n",
            "Iteration 1293, loss = 0.01094426\n",
            "Iteration 1294, loss = 0.01092353\n",
            "Iteration 1295, loss = 0.01087739\n",
            "Iteration 1296, loss = 0.01085445\n",
            "Iteration 1297, loss = 0.01089993\n",
            "Iteration 1298, loss = 0.01080492\n",
            "Iteration 1299, loss = 0.01078508\n",
            "Iteration 1300, loss = 0.01077095\n",
            "Iteration 1301, loss = 0.01077307\n",
            "Iteration 1302, loss = 0.01074670\n",
            "Iteration 1303, loss = 0.01074200\n",
            "Iteration 1304, loss = 0.01075515\n",
            "Iteration 1305, loss = 0.01070234\n",
            "Iteration 1306, loss = 0.01070841\n",
            "Iteration 1307, loss = 0.01069894\n",
            "Iteration 1308, loss = 0.01062961\n",
            "Iteration 1309, loss = 0.01060432\n",
            "Iteration 1310, loss = 0.01061178\n",
            "Iteration 1311, loss = 0.01061207\n",
            "Iteration 1312, loss = 0.01053360\n",
            "Iteration 1313, loss = 0.01055840\n",
            "Iteration 1314, loss = 0.01051857\n",
            "Iteration 1315, loss = 0.01050211\n",
            "Iteration 1316, loss = 0.01048716\n",
            "Iteration 1317, loss = 0.01048957\n",
            "Iteration 1318, loss = 0.01046346\n",
            "Iteration 1319, loss = 0.01049577\n",
            "Iteration 1320, loss = 0.01041034\n",
            "Iteration 1321, loss = 0.01052390\n",
            "Iteration 1322, loss = 0.01039083\n",
            "Iteration 1323, loss = 0.01036547\n",
            "Iteration 1324, loss = 0.01033322\n",
            "Iteration 1325, loss = 0.01034086\n",
            "Iteration 1326, loss = 0.01031726\n",
            "Iteration 1327, loss = 0.01029239\n",
            "Iteration 1328, loss = 0.01027152\n",
            "Iteration 1329, loss = 0.01025311\n",
            "Iteration 1330, loss = 0.01026190\n",
            "Iteration 1331, loss = 0.01021021\n",
            "Iteration 1332, loss = 0.01019086\n",
            "Iteration 1333, loss = 0.01021210\n",
            "Iteration 1334, loss = 0.01017373\n",
            "Iteration 1335, loss = 0.01020063\n",
            "Iteration 1336, loss = 0.01013633\n",
            "Iteration 1337, loss = 0.01011092\n",
            "Iteration 1338, loss = 0.01009571\n",
            "Iteration 1339, loss = 0.01009121\n",
            "Iteration 1340, loss = 0.01005859\n",
            "Iteration 1341, loss = 0.01005232\n",
            "Iteration 1342, loss = 0.01006539\n",
            "Iteration 1343, loss = 0.01002803\n",
            "Iteration 1344, loss = 0.01004561\n",
            "Iteration 1345, loss = 0.01000680\n",
            "Iteration 1346, loss = 0.00999093\n",
            "Iteration 1347, loss = 0.00995402\n",
            "Iteration 1348, loss = 0.00991717\n",
            "Iteration 1349, loss = 0.00990189\n",
            "Iteration 1350, loss = 0.00991875\n",
            "Iteration 1351, loss = 0.00990166\n",
            "Iteration 1352, loss = 0.00987130\n",
            "Iteration 1353, loss = 0.00985688\n",
            "Iteration 1354, loss = 0.00982987\n",
            "Iteration 1355, loss = 0.00987248\n",
            "Iteration 1356, loss = 0.00981649\n",
            "Iteration 1357, loss = 0.00978871\n",
            "Iteration 1358, loss = 0.00978331\n",
            "Iteration 1359, loss = 0.00975584\n",
            "Iteration 1360, loss = 0.00976022\n",
            "Iteration 1361, loss = 0.00974579\n",
            "Iteration 1362, loss = 0.00971271\n",
            "Iteration 1363, loss = 0.00969922\n",
            "Iteration 1364, loss = 0.00975436\n",
            "Iteration 1365, loss = 0.00969184\n",
            "Iteration 1366, loss = 0.00966975\n",
            "Iteration 1367, loss = 0.00962802\n",
            "Iteration 1368, loss = 0.00961845\n",
            "Iteration 1369, loss = 0.00959129\n",
            "Iteration 1370, loss = 0.00959759\n",
            "Iteration 1371, loss = 0.00957418\n",
            "Iteration 1372, loss = 0.00956627\n",
            "Iteration 1373, loss = 0.00954428\n",
            "Iteration 1374, loss = 0.00952489\n",
            "Iteration 1375, loss = 0.00951974\n",
            "Iteration 1376, loss = 0.00951541\n",
            "Iteration 1377, loss = 0.00949713\n",
            "Iteration 1378, loss = 0.00947193\n",
            "Iteration 1379, loss = 0.00947049\n",
            "Iteration 1380, loss = 0.00943606\n",
            "Iteration 1381, loss = 0.00942597\n",
            "Iteration 1382, loss = 0.00942643\n",
            "Iteration 1383, loss = 0.00941637\n",
            "Iteration 1384, loss = 0.00937587\n",
            "Iteration 1385, loss = 0.00935613\n",
            "Iteration 1386, loss = 0.00934630\n",
            "Iteration 1387, loss = 0.00936709\n",
            "Iteration 1388, loss = 0.00931615\n",
            "Iteration 1389, loss = 0.00931095\n",
            "Iteration 1390, loss = 0.00929329\n",
            "Iteration 1391, loss = 0.00930956\n",
            "Iteration 1392, loss = 0.00928991\n",
            "Iteration 1393, loss = 0.00924460\n",
            "Iteration 1394, loss = 0.00926251\n",
            "Iteration 1395, loss = 0.00921941\n",
            "Iteration 1396, loss = 0.00922542\n",
            "Iteration 1397, loss = 0.00918467\n",
            "Iteration 1398, loss = 0.00919187\n",
            "Iteration 1399, loss = 0.00916102\n",
            "Iteration 1400, loss = 0.00917433\n",
            "Iteration 1401, loss = 0.00911555\n",
            "Iteration 1402, loss = 0.00910816\n",
            "Iteration 1403, loss = 0.00916038\n",
            "Iteration 1404, loss = 0.00910651\n",
            "Iteration 1405, loss = 0.00907815\n",
            "Iteration 1406, loss = 0.00902304\n",
            "Iteration 1407, loss = 0.00903709\n",
            "Iteration 1408, loss = 0.00906603\n",
            "Iteration 1409, loss = 0.00899926\n",
            "Iteration 1410, loss = 0.00899389\n",
            "Iteration 1411, loss = 0.00897754\n",
            "Iteration 1412, loss = 0.00897247\n",
            "Iteration 1413, loss = 0.00897396\n",
            "Iteration 1414, loss = 0.00892413\n",
            "Iteration 1415, loss = 0.00890118\n",
            "Iteration 1416, loss = 0.00889750\n",
            "Iteration 1417, loss = 0.00893950\n",
            "Iteration 1418, loss = 0.00889890\n",
            "Iteration 1419, loss = 0.00887149\n",
            "Iteration 1420, loss = 0.00884633\n",
            "Iteration 1421, loss = 0.00886357\n",
            "Iteration 1422, loss = 0.00882734\n",
            "Iteration 1423, loss = 0.00880144\n",
            "Iteration 1424, loss = 0.00880123\n",
            "Iteration 1425, loss = 0.00878863\n",
            "Iteration 1426, loss = 0.00878657\n",
            "Iteration 1427, loss = 0.00875599\n",
            "Iteration 1428, loss = 0.00874341\n",
            "Iteration 1429, loss = 0.00876179\n",
            "Iteration 1430, loss = 0.00871310\n",
            "Iteration 1431, loss = 0.00870511\n",
            "Iteration 1432, loss = 0.00869436\n",
            "Iteration 1433, loss = 0.00868212\n",
            "Iteration 1434, loss = 0.00868890\n",
            "Iteration 1435, loss = 0.00865783\n",
            "Iteration 1436, loss = 0.00866980\n",
            "Iteration 1437, loss = 0.00864413\n",
            "Iteration 1438, loss = 0.00862899\n",
            "Iteration 1439, loss = 0.00861525\n",
            "Iteration 1440, loss = 0.00860149\n",
            "Iteration 1441, loss = 0.00871424\n",
            "Iteration 1442, loss = 0.00853921\n",
            "Iteration 1443, loss = 0.00858123\n",
            "Iteration 1444, loss = 0.00855807\n",
            "Iteration 1445, loss = 0.00855131\n",
            "Iteration 1446, loss = 0.00852232\n",
            "Iteration 1447, loss = 0.00849488\n",
            "Iteration 1448, loss = 0.00847161\n",
            "Iteration 1449, loss = 0.00847261\n",
            "Iteration 1450, loss = 0.00846507\n",
            "Iteration 1451, loss = 0.00843644\n",
            "Iteration 1452, loss = 0.00847938\n",
            "Iteration 1453, loss = 0.00843128\n",
            "Iteration 1454, loss = 0.00840564\n",
            "Iteration 1455, loss = 0.00838170\n",
            "Iteration 1456, loss = 0.00841299\n",
            "Iteration 1457, loss = 0.00840639\n",
            "Iteration 1458, loss = 0.00841496\n",
            "Iteration 1459, loss = 0.00834024\n",
            "Iteration 1460, loss = 0.00831958\n",
            "Iteration 1461, loss = 0.00830129\n",
            "Iteration 1462, loss = 0.00834089\n",
            "Iteration 1463, loss = 0.00837582\n",
            "Iteration 1464, loss = 0.00833101\n",
            "Iteration 1465, loss = 0.00827173\n",
            "Iteration 1466, loss = 0.00824572\n",
            "Iteration 1467, loss = 0.00826520\n",
            "Iteration 1468, loss = 0.00825485\n",
            "Iteration 1469, loss = 0.00822164\n",
            "Iteration 1470, loss = 0.00822760\n",
            "Iteration 1471, loss = 0.00819926\n",
            "Iteration 1472, loss = 0.00819797\n",
            "Iteration 1473, loss = 0.00817310\n",
            "Iteration 1474, loss = 0.00820065\n",
            "Iteration 1475, loss = 0.00817536\n",
            "Iteration 1476, loss = 0.00815210\n",
            "Iteration 1477, loss = 0.00813037\n",
            "Iteration 1478, loss = 0.00814533\n",
            "Iteration 1479, loss = 0.00810869\n",
            "Iteration 1480, loss = 0.00812237\n",
            "Iteration 1481, loss = 0.00808769\n",
            "Iteration 1482, loss = 0.00807935\n",
            "Iteration 1483, loss = 0.00804856\n",
            "Iteration 1484, loss = 0.00806129\n",
            "Iteration 1485, loss = 0.00814991\n",
            "Iteration 1486, loss = 0.00809816\n",
            "Iteration 1487, loss = 0.00805846\n",
            "Iteration 1488, loss = 0.00802761\n",
            "Iteration 1489, loss = 0.00802480\n",
            "Iteration 1490, loss = 0.00797973\n",
            "Iteration 1491, loss = 0.00799994\n",
            "Iteration 1492, loss = 0.00793876\n",
            "Iteration 1493, loss = 0.00793662\n",
            "Iteration 1494, loss = 0.00797928\n",
            "Iteration 1495, loss = 0.00790154\n",
            "Iteration 1496, loss = 0.00791954\n",
            "Iteration 1497, loss = 0.00790025\n",
            "Iteration 1498, loss = 0.00787852\n",
            "Iteration 1499, loss = 0.00787513\n",
            "Iteration 1500, loss = 0.00786527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-06, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-06, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-06, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZJSnfWQymtp",
        "outputId": "4798152c-70e0-4145-a73e-b520fac88b05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK-O4eihy0lO",
        "outputId": "7d7dd156-66fa-4dc6-b926-6c527fe4d5a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aODM790JzA-K",
        "outputId": "fa7c5be9-8174-4c53-9c40-3ee38eb7b19b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from yellowbrick.classifier import ConfusionMatrix\n",
        " cm = ConfusionMatrix(rede_neural_credit)\n",
        " cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        " cm.score(X_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "s7sqYMed2s5C",
        "outputId": "9cf7730d-8940-4124-f78e-fd390bd4b8cf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}